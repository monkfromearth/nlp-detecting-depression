{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXZ3lf8XEjzu"
   },
   "outputs": [],
   "source": [
    "# Uncomment to download the dataset\n",
    "\n",
    "#!wget \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\" -O data.zip\n",
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9h75CZhqErvM",
    "outputId": "e6cf6a88-b1d0-4331-f3e7-9ed2d51e45ec"
   },
   "outputs": [],
   "source": [
    "!pip install nltk wordcloud joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1ox8jShEtG5"
   },
   "outputs": [],
   "source": [
    "# for regular expressions\n",
    "import re \n",
    "from sklearn import re\n",
    "\n",
    "# for text manipulation\n",
    "import nltk \n",
    "import string \n",
    "import warnings\n",
    "\n",
    "# for data manipulation\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#importing different libraries for analysis, processing and classification\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from math import log, sqrt\n",
    "\n",
    "# vectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier #classification model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# performance evaluation criteria \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score  \n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chl94ydpEvor"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\") #ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "3jLJrRU1Ew02",
    "outputId": "7fc4fede-9773-407b-e2ba-66ccb5e95bc2"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')        \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKtm_q3yEymO"
   },
   "outputs": [],
   "source": [
    "def load_and_clean(filename, encoding=None):\n",
    "  # Loading dataset from csv\n",
    "  data = pd.read_csv(filename, encoding=encoding)\n",
    "  \n",
    "  # Setting custom columns from the dataset\n",
    "  DATASET_COLUMNS = [\"label\", \"ids\", \"date\", \"flag\", \"user\", \"message\"]\n",
    "  data.columns = DATASET_COLUMNS\n",
    "\n",
    "  # Dropping irrelevant columns\n",
    "  data.drop(['ids','date','flag','user'], axis = 1, inplace = True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2j5lEzO3QV8W"
   },
   "outputs": [],
   "source": [
    "def load_chunks(data, start=0, end=10000):\n",
    "  positive = data[data.label == 4].iloc[start:end, :]\n",
    "  negative = data[data.label == 0].iloc[start:end, :]\n",
    "  return pd.concat([ positive, negative ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sVS_kN4D9IV"
   },
   "outputs": [],
   "source": [
    "def normalizer(tweet, sw=True, stemmer=False, tfidf=None, vsw=None):\n",
    "    # Removing usernames\n",
    "    tweets = \" \".join(filter(lambda x: x[0] != '@' , tweet.split()))\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweets = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', tweets, flags=re.MULTILINE)\n",
    "    \n",
    "    # Removing everything but letters\n",
    "    tweets = re.sub(\"^a-zA-Z\", \"\", tweets)\n",
    "    \n",
    "    # Removing special HTML5 characters\n",
    "    tweets = tweets.replace(\"&lt;\", \"\")\n",
    "    tweets = tweets.replace(\"&amp;\", \"\")\n",
    "    tweets = tweets.replace(\"&quot;\", \"\")\n",
    "    \n",
    "    # Tokeninzing words\n",
    "    tweets = tweets.lower()\n",
    "    tweets = tweets.split()\n",
    "\n",
    "    if sw:\n",
    "      tweets = [word for word in tweets if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    if not stemmer:\n",
    "      lemma = WordNetLemmatizer()\n",
    "      tweets = [lemma.lemmatize(word) for word in tweets]\n",
    "    \n",
    "    if stemmer:\n",
    "      stemmer = SnowballStemmer('english')\n",
    "      tweets = [stemmer.stem(word) for word in tweets]\n",
    "    \n",
    "    # Forming the sentence again\n",
    "    tweets = \" \".join(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzUztx048nBr"
   },
   "outputs": [],
   "source": [
    "def create_vectorizer(data, tfidf=True, vsw=False, sw=None, stemmer=None):\n",
    "  pvectorizer = None\n",
    "  \n",
    "  if tfidf:\n",
    "    pvectorizer = TfidfVectorizer(stop_words='english' if vsw else None)\n",
    "\n",
    "  if not tfidf:\n",
    "    pvectorizer = CountVectorizer(stop_words='english' if vsw else None)\n",
    "\n",
    "  pvectorizer.fit_transform(data['message'])\n",
    "\n",
    "  save_vectorizer(pvectorizer, { 'tfidf':tfidf, 'vsw':vsw, 'sw':sw, 'stemmer':stemmer })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYu3uwABF9Gr"
   },
   "outputs": [],
   "source": [
    "def vectorizer(message, tfidf=True, vsw=False, sw=None, stemmer=None):\n",
    "  pvectorizer = load_vectorizer({ 'tfidf':tfidf, 'vsw':vsw, 'sw':sw, 'stemmer':stemmer })\n",
    "\n",
    "  cv = pvectorizer.transform(message)\n",
    "\n",
    "  return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VtEmGOfF-gU"
   },
   "outputs": [],
   "source": [
    "def normalize_and_vectorize(data, **kwargs):\n",
    "\n",
    "  data.message = data.message.apply(lambda tweet: normalizer(tweet, **kwargs))\n",
    "\n",
    "  cv = vectorizer(data.message, **kwargs)\n",
    "\n",
    "  return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PmrtAWiZYMK"
   },
   "outputs": [],
   "source": [
    "def load_name_from_configs(configs):\n",
    "  return [\n",
    "    \"STM\" if configs['stemmer'] else \"LEM\",\n",
    "    \"TFI\" if configs['tfidf'] else \"BOW\",\n",
    "    \"STW\" if configs['sw'] else \"NSW\",\n",
    "    \"VSW\" if configs['vsw'] else \"NVS\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsPydvDX3ghX"
   },
   "outputs": [],
   "source": [
    "def load_vectorizer(configs):\n",
    "  try:\n",
    "    name = load_name_from_configs(configs)\n",
    "    return joblib.load(\"VCT.{}-{}-{}-{}.pkl\".format(*name))\n",
    "  except Exception as e: pass\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUREeyee3t75"
   },
   "outputs": [],
   "source": [
    "def save_vectorizer(cv, configs):\n",
    "  try:\n",
    "    name = load_name_from_configs(configs)\n",
    "    joblib.dump(cv, \"VCT.{}-{}-{}-{}.pkl\".format(*name))\n",
    "  except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnbx0WEXGDct"
   },
   "outputs": [],
   "source": [
    "def load_models(configs):\n",
    "  try:\n",
    "    name = load_name_from_configs(configs)\n",
    "    lr = joblib.load(\"LGR.{}-{}-{}-{}.model\".format(*name))\n",
    "    mnb = joblib.load(\"MNB.{}-{}-{}-{}.model\".format(*name))\n",
    "    svc = joblib.load(\"SVC.{}-{}-{}-{}.model\".format(*name))\n",
    "    sgd = joblib.load(\"SGD.{}-{}-{}-{}.model\".format(*name))\n",
    "    return (lr, mnb, svc, sgd)\n",
    "  except Exception as e: pass\n",
    "  return (None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urbp0qWzF_ss"
   },
   "outputs": [],
   "source": [
    "def save_models(configs, lr, mnb, svc, sgd):\n",
    "  try:\n",
    "    name = load_name_from_configs(configs)\n",
    "    joblib.dump(lr, \"LGR.{}-{}-{}-{}.model\".format(*name))\n",
    "    joblib.dump(mnb, \"MNB.{}-{}-{}-{}.model\".format(*name))\n",
    "    joblib.dump(svc, \"SVC.{}-{}-{}-{}.model\".format(*name))\n",
    "    joblib.dump(sgd, \"SGD.{}-{}-{}-{}.model\".format(*name))\n",
    "  except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3coUmWIdGFwy"
   },
   "outputs": [],
   "source": [
    "def train_test_save(cv, label, start, end, configs):\n",
    "  X_train,X_test,y_train,y_test = train_test_split(cv, label , test_size=.2, stratify=label, random_state=6174)\n",
    "  \n",
    "  lr, mnb, svc, sgd = load_models(configs)\n",
    "\n",
    "  # Logistic Regression\n",
    "  if lr == None:\n",
    "    lr = LogisticRegression()\n",
    "  lr.fit(X_train,y_train)\n",
    "  predictions_lr = lr.predict(X_test)\n",
    "  ac_lr = accuracy_score(y_test, predictions_lr)\n",
    "  print(\"LR: \", ac_lr)\n",
    "  \n",
    "  # Multinomial Naive Bayes\n",
    "  if mnb == None:\n",
    "    mnb = MultinomialNB()\n",
    "  mnb.fit(X_train,y_train)\n",
    "  predictions_mnb = mnb.predict(X_test)\n",
    "  ac_mnb = accuracy_score(y_test, predictions_mnb)\n",
    "  print(\"MNB: \", ac_mnb)\n",
    "\n",
    "  # Support Vector Machine\n",
    "  if svc == None:\n",
    "    svc = SVC()\n",
    "  svc.fit(X_train,y_train)\n",
    "  predictions_svc = svc.predict(X_test)\n",
    "  ac_svc = accuracy_score(y_test, predictions_svc)\n",
    "  print(\"SVC: \", ac_svc)\n",
    "  \n",
    "  # Stochastic Gradient Descent\n",
    "  if sgd == None:\n",
    "    sgd = SGDClassifier()\n",
    "  sgd.fit(X_train,y_train)\n",
    "  predictions_sgd = sgd.predict(X_test)\n",
    "  ac_sgd = accuracy_score(y_test, predictions_sgd)\n",
    "  print(\"SGD: \", ac_sgd)\n",
    "\n",
    "  d = configs.copy()\n",
    "  d.update({\n",
    "      'start':start,\n",
    "      'end':end,\n",
    "      'lr':ac_lr,\n",
    "      'mnb':ac_mnb,\n",
    "      'svc':ac_svc,\n",
    "      'sgd':ac_sgd\n",
    "  })\n",
    "\n",
    "  save_models(configs, lr, mnb, svc, sgd)\n",
    "\n",
    "  return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9xvMzfvGHXK"
   },
   "outputs": [],
   "source": [
    "def start_training():\n",
    "\n",
    "  config = {\n",
    "      \"sw\":True,\n",
    "      \"vsw\":True,\n",
    "      \"stemmer\":True,\n",
    "      \"tfidf\":True\n",
    "  }\n",
    "\n",
    "  \"\"\"\n",
    "    load_and_clean(filename) -> pd.DataFrame\n",
    "    load_chunks(data, start=0, end=10000)\n",
    "    normalizer(tweet, sw=True, stemmer=False)\n",
    "    vectorizer(message, tfidf=True, vsw=False)\n",
    "    normalize_and_vectorize(data, **kwargs) -> cv\n",
    "    load_name_from_configs(configs)\n",
    "    load_models(configs)\n",
    "    save_models(configs, lr, mnb, svc, sgd)\n",
    "    train_test_save(cv, label, start, end, config)\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    ledger = pd.read_csv('ledger.csv')\n",
    "  except Exception as e:\n",
    "    ledger = pd.DataFrame(columns=['start', 'end', 'sw', 'vsw', 'stemmer', 'tfidf', 'lr', 'mnb', 'svc', 'sgd' ])\n",
    "\n",
    "\n",
    "  data = load_and_clean('training.1600000.processed.noemoticon.csv', 'latin1')\n",
    "\n",
    "  create_vectorizer(data, **config)\n",
    "\n",
    "  start = (ledger.iloc[ledger.shape[0] - 1]['end']) if (ledger.shape[0] > 0) else 0\n",
    "  end = 0 # this the amount of data, the quantity is x2 for the data\n",
    "  # as both positive and negative are taken equally to remove class imbalance\n",
    "\n",
    "  for i in range(start, end, 10000):\n",
    "\n",
    "    print(\"Start, End: \", i, i+10000)\n",
    "\n",
    "    df = load_chunks(data, i, i+10000)\n",
    "    \n",
    "    cv = normalize_and_vectorize(df, **config)\n",
    "    d = train_test_save(cv, df['label'], i, i+10000, config)\n",
    "    \n",
    "    ledger = ledger.append(d, ignore_index=True)\n",
    "    ledger.to_csv('ledger.csv', index=False)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVFQAj77CC7C"
   },
   "outputs": [],
   "source": [
    "def test_on_dataset(start=600_000, end=800_000):\n",
    "\n",
    "  config = {\n",
    "      \"sw\":True,\n",
    "      \"vsw\":True,\n",
    "      \"stemmer\":True,\n",
    "      \"tfidf\":True\n",
    "  }\n",
    "  \n",
    "  lr, mnb, svc, sgd = load_models(config)\n",
    "  testdf = load_chunks(data, 600_000, 800_000)\n",
    "  cv = normalize_and_vectorize(testdf, **config)\n",
    "\n",
    "  test_lr = lr.predict(cv)\n",
    "  print(\"LR: \", accuracy_score(testdf.label, test_lr))\n",
    "  print(confusion_matrix(testdf.label, test_lr))\n",
    "  print(classification_report(testdf.label, test_lr))\n",
    "\n",
    "  test_mnb = mnb.predict(cv)\n",
    "  print(\"MNB: \", accuracy_score(testdf.label, test_mnb))\n",
    "  print(confusion_matrix(testdf.label, test_mnb))\n",
    "  print(classification_report(testdf.label, test_mnb))\n",
    "\n",
    "  test_svc = svc.predict(cv)\n",
    "  print(\"SVC: \", accuracy_score(testdf.label, test_svc))\n",
    "  print(confusion_matrix(testdf.label, test_svc))\n",
    "  print(classification_report(testdf.label, test_svc))\n",
    "\n",
    "  test_sgd = sgd.predict(cv)\n",
    "  print(\"SGD: \", accuracy_score(testdf.label, test_sgd))\n",
    "  print(confusion_matrix(testdf.label, test_sgd))\n",
    "  print(classification_report(testdf.label, test_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8NX6p9wJf4m"
   },
   "outputs": [],
   "source": [
    "def classify_with_models(text, **kwargs):\n",
    "\n",
    "  config = {\n",
    "      \"sw\":True,\n",
    "      \"vsw\":True,\n",
    "      \"stemmer\":True,\n",
    "      \"tfidf\":True\n",
    "  }\n",
    "\n",
    "  string = normalizer(text, **config)\n",
    "  cv = vectorizer([ string ], **config)\n",
    "\n",
    "  lr, mnb, svc, sgd = load_models(config)\n",
    "\n",
    "  print(text)\n",
    "  print(\"LR:\", \"Happy\" if lr.predict(cv)[0] == 4 else \"Depressed\", end=\"\\t\")\n",
    "  print(\"MNB:\", \"Happy\" if mnb.predict(cv)[0] == 4 else \"Depressed\", end=\"\\t\")\n",
    "  print(\"SVC:\", \"Happy\" if svc.predict(cv)[0] == 4 else \"Depressed\", end=\"\\t\")\n",
    "  print(\"SGD:\", \"Happy\" if sgd.predict(cv)[0] == 4 else \"Depressed\", end=\"\\t\")\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLx_xD3OPvPL"
   },
   "source": [
    "# Demo with Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment only if you have the dataset downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZd7J_tN3fF5"
   },
   "outputs": [],
   "source": [
    "# data = load_and_clean('training.1600000.processed.noemoticon.csv', 'latin1')\n",
    "# tweets = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "Dpaa7YLE4pyI",
    "outputId": "5d129acb-0f56-463a-9bb2-dc748d3e0a24"
   },
   "outputs": [],
   "source": [
    "# depressive_words = ' '.join(list(tweets[tweets['label'] == 0]['message']))\n",
    "# depressive_wc = WordCloud(width = 512,height = 512, collocations=False, colormap=\"Blues\").generate(depressive_words)\n",
    "# plt.figure(figsize = (8, 6), facecolor = 'k')\n",
    "# plt.imshow(depressive_wc)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad = 0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "8EOGVEpY4tVV",
    "outputId": "82aec464-3991-480d-b0a4-8bf2e58d588d"
   },
   "outputs": [],
   "source": [
    "# positive_words = ' '.join(list(tweets[tweets['label'] == 4]['message']))\n",
    "# positive_wc = WordCloud(width = 512,height = 512, collocations=False, colormap=\"Blues\").generate(positive_words)\n",
    "# plt.figure(figsize = (8, 6), facecolor = 'k')\n",
    "# plt.imshow(positive_wc)\n",
    "# plt.axis('off'), \n",
    "# plt.tight_layout(pad = 0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wafPrDwXS_0D"
   },
   "source": [
    "## Depressive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "vn36OntzLgpe",
    "outputId": "c5593f8f-44e6-46f8-ef9d-af0a9412e115"
   },
   "outputs": [],
   "source": [
    "classify_with_models('Lately I have been feeling unsure of myself as a person & an artist')\n",
    "classify_with_models('Extreme sadness, lack of energy, hopelessness')\n",
    "classify_with_models('Hi hello depression and anxiety are the worst')\n",
    "classify_with_models('I am officially done with @kanyewest')\n",
    "classify_with_models('Feeling down...')\n",
    "classify_with_models('My depression will not let me work out')\n",
    "classify_with_models(\"I wish life could be better\")\n",
    "classify_with_models(\"I feel like attempting suicide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "c94quCzw2Od2",
    "outputId": "e9728738-0164-4c26-98fd-fb417b40e5a1"
   },
   "outputs": [],
   "source": [
    "classify_with_models(\"I am happy to be alone and lonely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0d0i8_PbGZA8",
    "outputId": "7ca8e8a3-1bb8-4ca2-8b78-4491380123d2"
   },
   "outputs": [],
   "source": [
    "classify_with_models('lately i have been feeling very lonely and depressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "qGoMDNq7RA2e",
    "outputId": "2847c588-73e7-4924-c7b9-19cb62ee140b"
   },
   "outputs": [],
   "source": [
    "classify_with_models('Loving how me and my lovely partner is talking about what we want.')\n",
    "classify_with_models('Very rewarding when a patient hugs you and tells you they feel great after changing the diet and daily habits')\n",
    "classify_with_models('Happy Thursday everyone. Thought today was Wednesday so super happy tomorrow is Friday yayyyyy')\n",
    "classify_with_models('It’s the little things that make me smile. Got our new car today and this arrived with it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "O8YHPCYV7mRB",
    "outputId": "7120acee-e789-436a-c978-4268a2587705"
   },
   "outputs": [],
   "source": [
    "classify_with_models('I am not satisfied with their teaching')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQlXpp7NXGq5"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Testing and Reports\n",
    "\n",
    "Run with *399,999* data points.\n",
    "\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "**Accuracy:**  0.7275868189670475\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "- | Predicted No | Predicted Yes\n",
    "--- | --- | ---\n",
    "Actual No | 136825 | 63174\n",
    "Actual Yes | 45791 | 154209\n",
    "\n",
    "![LR Normalized Confusion Matrix](https://drive.google.com/uc?id=1FIjDLMdq1X1dlDeYI-4AK5LuBVIJvH3U)\n",
    "\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "- | precision | recall | f1-score | support\n",
    "--- | --- | --- | --- | ---\n",
    "0 | 0.75 | 0.68 | 0.72 | 199999\n",
    "4 | 0.71 | 0.77 | 0.74 | 200000\n",
    "accuracy | | | 0.73 | 399999\n",
    "macro avg | 0.73 | 0.73 | 0.73 | 399999\n",
    "weighted avg | 0.73 | 0.73 | 0.73 | 399999\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "\n",
    "\n",
    "**Accuracy**:  0.7143142857857144\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "- | Predicted No | Predicted Yes\n",
    "--- | --- | ---\n",
    "Actual No | 145915 | 54084\n",
    "Actual Yes | 60190 | 139810\n",
    "\n",
    "![MNB Normalized Confusion Matrix](https://drive.google.com/uc?id=1XcY4eKrPGYVAheO0ZMUQqi-_DiU0Z_6j)\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "- | precision | recall | f1-score | support\n",
    "--- | --- | --- | --- | ---\n",
    "0 | 0.71 | 0.73 | 0.72 | 199999\n",
    "4 | 0.72 | 0.70 | 0.71 | 200000\n",
    "accuracy | | | 0.71 | 399999\n",
    "macro avg | 0.71 | 0.71 | 0.71 | 399999\n",
    "weighted avg | 0.71 | 0.71 | 0.71 | 399999\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Support Vector Classifier\n",
    "\n",
    "\n",
    "**Accuracy**:  0.7306768266920667\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "- | Predicted No | Predicted Yes\n",
    "--- | --- | ---\n",
    "Actual No | 136128 | 63871\n",
    "Actual Yes | 43858 | 156142\n",
    "\n",
    "![SVC Normalized Confusion Matrix](https://drive.google.com/uc?id=171NyDpoH2bkuSXWUjQhph5ENFRS8qxB3)\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "- | precision | recall | f1-score | support\n",
    "--- | --- | --- | --- | ---\n",
    "0 | 0.76 | 0.68 | 0.72 | 199999\n",
    "4 | 0.71 | 0.78 | 0.74 | 200000\n",
    "accuracy | | | 0.73 | 399999\n",
    "macro avg | 0.73 | 0.73 | 0.73 | 399999\n",
    "weighted avg | 0.73 | 0.73 | 0.73 | 399999\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "**Accuracy**:  0.7262618156545392\n",
    "\n",
    "**Confusion Matrix**:\n",
    "\n",
    "- | Predicted No | Predicted Yes\n",
    "--- | --- | ---\n",
    "Actual No | 133102 | 66897\n",
    "Actual Yes | 42598 | 15740\n",
    "\n",
    "![SGD Normalized Confusion Matrix](https://drive.google.com/uc?id=1FM1qY9Ky-UgFxz0GY2nNE26IAWJroitx)\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "- | precision | recall | f1-score | support\n",
    "--- | --- | --- | --- | ---\n",
    "0 | 0.76 | 0.67 | 0.71 | 199999\n",
    "4 | 0.70 | 0.79 | 0.74 | 200000\n",
    "accuracy | | | 0.73 | 399999\n",
    "macro avg | 0.73 | 0.73 | 0.73 | 399999\n",
    "weighted avg | 0.73 | 0.73 | 0.73 | 399999\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Demo Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
